# convert the integers back to words
word_index_df = data.frame(
word = names(word_index),
idx = unlist(word_index, use.names = FALSE),
stringsAsFactors = FALSE
)
decode_review = function(word_idx) {
paste(map(word_idx, function(number) word_index_df %>%
filter(idx == number) %>%
select(word) %>%
pull()),
collapse = ' ')
}
decode_review(train_data[[1]])
train_data = pad_sequences(
train_data,
value = 0,
padding = "post",
maxlen = 256
)
test_data = pad_sequences(
test_data,
value = 0,
padding = "post",
maxlen = 256
)
vocab_size = 10000
model = keras_model_sequential()
model %>%
layer_embedding(input_dim = vocab_size, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% summary()
# LOSS FUNCTION AND OPTIMIZER
model %>% compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = c("accuracy")
)
# create a validation set
x_val <- train_data[1:10000, ]
partial_x_train <- train_data[10001:nrow(train_data), ]
y_val <- train_labels[1:10000]
partial_y_train <- train_labels[10001:length(train_labels)]
length(x_val)
length(y_val)
length(partial_x_train)
length(partial_y_train)
train_data = imdb$train$x
train_labels = imdb$train$y
test_data = imdb$test$x
test_labels = imdb$test$y
# the dataset comes with an index mapping words to integers
# which needs to be downloaded separately
word_index = dataset_imdb_word_index()
# Explore the data
dim(train_data)
str(train_data)
str(train_labels)
# convert the integers back to words
word_index_df = data.frame(
word = names(word_index),
idx = unlist(word_index, use.names = FALSE),
stringsAsFactors = FALSE
)
decode_review = function(word_idx) {
paste(map(word_idx, function(number) word_index_df %>%
filter(idx == number) %>%
select(word) %>%
pull()),
collapse = ' ')
}
decode_review(train_data[[1]])
train_data = pad_sequences(
train_data,
value = 0,
padding = "post",
maxlen = 256
)
test_data = pad_sequences(
test_data,
value = 0,
padding = "post",
maxlen = 256
)
vocab_size = 10000
model = keras_model_sequential()
model %>%
layer_embedding(input_dim = vocab_size, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
model %>% summary()
# LOSS FUNCTION AND OPTIMIZER
model %>% compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = c("accuracy")
)
length(train_data)
dim(train_data)
# create a validation set
x_val <- train_data[1:10000, ]
partial_x_train <- train_data[10001:nrow(train_data), ]
y_val <- train_labels[1:10000]
partial_y_train <- train_labels[10001:length(train_labels)]
dim(x_val)
dim(y_val)
dim(partial_x_train)
dim(partial_y_train)
# train the model
history = model %>% fit(
partial_x_train, partial_y_train,
epochs = 40,
batch_size = 512,
validation_data = list(x_val, y_val),
verbose = 1
)
# evaluate the model
results = model %>% evaluate(test_data, test_labels)
results
# plot training history
plot(history)
library(keras)
library(keras)
# import data
boston_housing = dataset_boston_housing()
#
c(train_data, train_labels) %<% boston_housing$train
#
c(train_data, train_labels) %<-% boston_housing$train
c(test_data, test_labels) %<-% boston_housing$test
# explore data
dim(train_data)
dim(train_labels)
# Let's add column names for better data inspection
library(tibble)
column_names <- c('CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',
'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT')
train_df = as_tibble(train_data)
colnames(train_df) = column_names
train_df
# normalize features
train_data = scale(train_data)
attr(train_data, "scaled:center")
attributes(train_data)
col_stddevs_train = attr(train_data, "scaled:scale")
test_data = scale(test_data, center = col_means_train, scale = col_stddevs_train)
# use means and standard deviations from training set to normalize
# test set
col_means_train = attr(train_data, "scaled:center")
col_stddevs_train = attr(train_data, "scaled:scale")
test_data = scale(test_data, center = col_means_train, scale = col_stddevs_train)
train_data[1, ]
# CREATE THE MODEL
build_model = function() {
model = keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu',
input_shape = dim(train_data)[2]) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 1)
model %>% compile(
loss = 'mse',
optimizer = optimizer_rmsprop(),
metrics = list('mean_absolute_error')
)
model
}
model = build_model()
model %>% summary()
# TRAIN THE MODEL
# display training progress by printing a single dot for each completed epoch
print_dot_callback = callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 80 == 0) cat('\n')
cat('.')
}
)
epochs = 500
# fit the model and store training stats
history = model %>% fit(
train_data, train_labels,
epochs = epochs,
validation_split = 0.2,
verbose = 0,
callbacks = list(print_dot_callback)
)
library(ggplot2)
plot(history)
plot(history, metrics = "mean_absolute_error", smooth = FALSE)
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(ylim = c(0, 10))
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(ylim = c(0, 5))
elary_stop = callback_early_stopping(monitor = "val_loss", patience = 20)
model = build_model()
elary_stop = callback_early_stopping(monitor = "val_loss", patience = 20)
# TRAIN THE MODEL
# display training progress by printing a single dot for each completed epoch
print_dot_callback = callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 80 == 0) cat('\n')
cat('>.<')
}
)
epochs = 500
elary_stop = callback_early_stopping(monitor = "val_loss", patience = 20)
model = build_model()
history = model %>% fit(
train_data, train_labels,
epochs = epochs,
validation_split = 0.2,
verbose = 0,
callbacks = list(early_stop, print_dot_callback)
)
earlyy_stop = callback_early_stopping(monitor = "val_loss", patience = 20)
early_stop = callback_early_stopping(monitor = "val_loss", patience = 20)
model = build_model()
history = model %>% fit(
train_data, train_labels,
epochs = epochs,
validation_split = 0.2,
verbose = 0,
callbacks = list(early_stop, print_dot_callback)
)
plot(history, metrics = 'mean_absolute_error', smooth = TRUE) +
coord_cartesian(xlim = c(0, 150), ylim = c(0, 5))
plot(history, metrics = 'mean_absolute_error', smooth = FALSE) +
coord_cartesian(xlim = c(0, 150), ylim = c(0, 5))
plot(history, metrics = 'mean_absolute_error', smooth = FALSE) +
coord_cartesian(xlim = c(0, 200), ylim = c(0, 5))
# evaluate model's performance with test set
model %>% evaluate(test_data, test_labels)
# predict
model %>% predict(test_data)
# predict
pred = model %>% predict(test_data)
plot(pred, test_labels)
library(keras)
library(dplyr)
library(ggplot2)
library(tidyr)
library(tibble)
# download the IMDB dataset
num_words = 10000
imdb = dataset_imdb(num_words = num_words)
train_data = imdb$train$x
train_labels = imdb$train$y
test_data = imdb$test$x
test_labels = imdb$test$y
# multi-hot-encoding the lists
multi_hot_sequences = function(sequences, dimension) {
multi_hot = matrix(0, nrow = length(sequences), ncol = dimension)
for (i in 1:length(sequences)) {
multi_hot[i, sequences[[i]]] = 1
}
multi_hot
}
train_data = multi_hot_sequences(train_data, num_words)
test_data = multi_hot_sequences(test_data, num_words)
dim(train_data)
dim(test_data)
train_data = imdb$train$x
train_labels = imdb$train$y
test_data = imdb$test$x
test_labels = imdb$test$y
length(test_data)
length(train_data)
# multi-hot-encoding the lists
multi_hot_sequences = function(sequences, dimension) {
multi_hot = matrix(0, nrow = length(sequences), ncol = dimension)
for (i in 1:length(sequences)) {
multi_hot[i, sequences[[i]]] = 1
}
multi_hot
}
train_data = multi_hot_sequences(train_data, num_words)
test_data = multi_hot_sequences(test_data, num_words)
dim(train_data)
dim(test_data)
train_data[1, ]
which(train_data[1, ])
which(train_data[1, ] == 1)
# look at one of the resulting multi-hot vectors
plot(x = 1:10000)
# look at one of the resulting multi-hot vectors
plot(x = 1:10000, yaxt='n')
# look at one of the resulting multi-hot vectors
plot(x = 1:10000, yaxt='n', type='n')
abline(a=which(train_data[1, ] == 1))
abline(a=which(train_data[1, ] == 1), b=0)
abline(b=which(train_data[1, ] == 1), a=0)
b=which(train_data[1, ] == 1)
b
# look at one of the resulting multi-hot vectors
plot(x = 1:10000, yaxt='n', type='n')
abline(h=bwhich(train_data[1, ] == 1))
abline(h=which(train_data[1, ] == 1))
abline(v=which(train_data[1, ] == 1))
# look at one of the resulting multi-hot vectors
plot(x = 1:10000, yaxt='n', type='n')
abline(v=which(train_data[1, ] == 1))
?plot
# look at one of the resulting multi-hot vectors
plot(x = 1:10000, yaxt='n', type='n', frame.plot = FALSE)
abline(v=which(train_data[1, ] == 1))
baseline_model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = list('accuracy')
)
# CREATE A BASELINE MODEL
baseline_model = keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = 10000) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = "sigmoid")
baseline_model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = list('accuracy')
)
baseline_model %>% summary()
baseline_history = baseline_model %>% fit(
train_data, train_labels,
epochs = 20,
batch_size = 512,
validation_data = list(test_data, test_labels),
verbose = 2
)
# CREATE A SMALLER MODEL
smaller_model = keras_model_sequential() %>%
layer_dense(units = 4, activation = 'relu', input_shape = 10000) %>%
layer_dense(units = 4, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
smaller_model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = list('accuracy')
)
smaller_model %>% summary()
smaller_history = smaller_model %>% fit(
train_data, train_labels,
epochs = 20,
batch_size = 512,
validation_data = list(test_data, test_labels),
verbose = 2
)
# CREATE A BIGGER MODEL
bigger_model = keras_model_sequential() %>%
layer_dense(units = 512, activation = 'relu', input_shape = 10000) %>%
layer_dense(units = 512, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
bigger_model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = list('accuracy')
)
bigger_model %>% summary()
bigger_history = bigger_model %>% fit(
train_data, train_labels,
epochs = 20,
batch_size = 512,
validation_data = list(test_data, test_labels),
verbose = 2
)
# PLOT THE TRINING AND VALIDATION LOSS
compare_cx = data.frame(
baseline_train = baseline_history$metrics$loss,
baseline_val = baseline_history$metrics$val_loss,
smaller_train =smaller_history$metrics$loss,
smaller_val = smaller_history$metrics$val_loss,
bigger_train = bigger_history$metrics$loss,
bigger_val = bigger_history$metrics$val_loss
)
compare_cx
plot()
plot(1:nrow(compare_cx), compare_cx[, 1])
plot(1:nrow(compare_cx), compare_cx[, 1], col = 1)
plot(1:nrow(compare_cx), compare_cx[, 1], col = 'red')
plot(1:nrow(compare_cx), compare_cx[, 1], col = 'red', type = 'l')
gather(compare_cx, key='type', value = 'loss')
# PLOT THE TRINING AND VALIDATION LOSS
compare_cx = data.frame(
baseline_train = baseline_history$metrics$loss,
baseline_val = baseline_history$metrics$val_loss,
smaller_train =smaller_history$metrics$loss,
smaller_val = smaller_history$metrics$val_loss,
bigger_train = bigger_history$metrics$loss,
bigger_val = bigger_history$metrics$val_loss,
epochs = 1:20
)
gather(compare_cx, key='type', value = 'loss', -epochs)
# PLOT THE TRINING AND VALIDATION LOSS
compare_cx = data.frame(
baseline_train = baseline_history$metrics$loss,
baseline_val = baseline_history$metrics$val_loss,
smaller_train =smaller_history$metrics$loss,
smaller_val = smaller_history$metrics$val_loss,
bigger_train = bigger_history$metrics$loss,
bigger_val = bigger_history$metrics$val_loss,
epochs = 1:20
) %>%
gather(key='type', value = 'loss', -epochs)
compare_cx
stringr::str_split(compare_cx$type, '_')
stringr::str_split(compare_cx$type, '_', n = 2)
apply(compare_cx, function(x) stringr::str_split(x)[[1]])
tpply(compare_cx, FUN = function(x) stringr::str_split(x)[[1]])
sapply(compare_cx, FUN = function(x) stringr::str_split(x)[[1]])
sapply(compare_cx, FUN = function(x) stringr::str_split(x, '_')[[1]])
sapply(compare_cx$type, FUN = function(x) stringr::str_split(x, '_')[[1]])
tapply(compare_cx$type, FUN = function(x) stringr::str_split(x, '_')[[1]])
sapply(compare_cx$type, FUN = function(x) stringr::str_split(x, '_')[[1]])
stringr::str_split(compare_cx, '_')
stringr::str_split(compare_cx$type, '_')
as.data.frame(stringr::str_split(compare_cx$type, '_'))
t(as.data.frame(stringr::str_split(compare_cx$type, '_')))
stringr::str_split(compare_cx$type, '_'))
stringr::str_split(compare_cx$type, '_')))
stringr::str_split(compare_cx$type, '_')
stringr::str_split(compare_cx$type, '_', 1)
unlist(stringr::str_split(compare_cx$type, '_', 1))
compare_cx$model = unlist(stringr::str_split(compare_cx$type, '_', 1))
unlist(stringr::str_split(compare_cx$type, '_', 1))
unlist(stringr::str_split(compare_cx$type, '*_', 1))
unlist(stringr::str_split(compare_cx$type, '\\*_', 1))
unlist(stringr::str_split(compare_cx$type, '\*_', 1))
unlist(stringr::str_split(compare_cx$type, '\\._', 1))
library(stringr)
str_replace('\\.+_', compare_cx$type)
str_replace(compare_cx$type, '\\.+_', '')
str_replace(compare_cx$type, '\\.\\+_', '')
str_replace(compare_cx$type, '_', '')
str_replace(compare_cx$type, '.+_', '')
# PLOT THE TRINING AND VALIDATION LOSS
compare_cx = data.frame(
baseline_train = baseline_history$metrics$loss,
baseline_val = baseline_history$metrics$val_loss,
smaller_train =smaller_history$metrics$loss,
smaller_val = smaller_history$metrics$val_loss,
bigger_train = bigger_history$metrics$loss,
bigger_val = bigger_history$metrics$val_loss,
epochs = 1:20
) %>%
gather(key='type', value = 'loss', -epochs)
compare_cx$model = str_replace(compare_cx$type, '_.+', '')
compare_cx$type = str_replace(compare_cx$type, '.+_', '')
head(compare_cx)
ggplot(compare_cx, aes(x=epochs, y=loss, color=type, line_type=model)) +
geom_line()
model
compare_cx$model
ggplot(compare_cx, aes(x=epochs, y=loss, color=model, linetype=type)) +
geom_line()
# regularization
l2_model <-
keras_model_sequential() %>%
layer_dense(units = 16, activation = "relu", input_shape = 10000,
kernel_regularizer = regularizer_l2(l = 0.001)) %>%
layer_dense(units = 16, activation = "relu",
kernel_regularizer = regularizer_l2(l = 0.001)) %>%
layer_dense(units = 1, activation = "sigmoid")
l2_model %>% compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = list("accuracy")
)
l2_history <- l2_model %>% fit(
train_data,
train_labels,
epochs = 20,
batch_size = 512,
validation_data = list(test_data, test_labels),
verbose = 2
)
# add dropout
dropout_model <-
keras_model_sequential() %>%
layer_dense(units = 16, activation = "relu", input_shape = 10000) %>%
layer_dropout(0.6) %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.6) %>%
layer_dense(units = 1, activation = "sigmoid")
dropout_model %>% compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = list("accuracy")
)
dropout_history <- dropout_model %>% fit(
train_data,
train_labels,
epochs = 20,
batch_size = 512,
validation_data = list(test_data, test_labels),
verbose = 2
)
# PLOT THE TRINING AND VALIDATION LOSS
compare_cx = data.frame(
baseline_train = baseline_history$metrics$loss,
baseline_val = baseline_history$metrics$val_loss,
l2_train =l2_history$metrics$loss,
l2_val = l2_history$metrics$val_loss,
dropout_train = dropout_history$metrics$loss,
dropout_val = dropout_history$metrics$val_loss,
epochs = 1:20
) %>%
gather(key='type', value = 'loss', -epochs)
compare_cx$model = str_replace(compare_cx$type, '_.+', '')
compare_cx$type = str_replace(compare_cx$type, '.+_', '')
ggplot(compare_cx, aes(x=epochs, y=loss, color=model, linetype=type)) +
geom_line()
