results
# plot training history
plot(history)
library(keras)
library(keras)
# import data
boston_housing = dataset_boston_housing()
#
c(train_data, train_labels) %<% boston_housing$train
#
c(train_data, train_labels) %<-% boston_housing$train
c(test_data, test_labels) %<-% boston_housing$test
# explore data
dim(train_data)
dim(train_labels)
# Let's add column names for better data inspection
library(tibble)
column_names <- c('CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',
'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT')
train_df = as_tibble(train_data)
colnames(train_df) = column_names
train_df
# normalize features
train_data = scale(train_data)
attr(train_data, "scaled:center")
attributes(train_data)
col_stddevs_train = attr(train_data, "scaled:scale")
test_data = scale(test_data, center = col_means_train, scale = col_stddevs_train)
# use means and standard deviations from training set to normalize
# test set
col_means_train = attr(train_data, "scaled:center")
col_stddevs_train = attr(train_data, "scaled:scale")
test_data = scale(test_data, center = col_means_train, scale = col_stddevs_train)
train_data[1, ]
# CREATE THE MODEL
build_model = function() {
model = keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu',
input_shape = dim(train_data)[2]) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 1)
model %>% compile(
loss = 'mse',
optimizer = optimizer_rmsprop(),
metrics = list('mean_absolute_error')
)
model
}
model = build_model()
model %>% summary()
# TRAIN THE MODEL
# display training progress by printing a single dot for each completed epoch
print_dot_callback = callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 80 == 0) cat('\n')
cat('.')
}
)
epochs = 500
# fit the model and store training stats
history = model %>% fit(
train_data, train_labels,
epochs = epochs,
validation_split = 0.2,
verbose = 0,
callbacks = list(print_dot_callback)
)
library(ggplot2)
plot(history)
plot(history, metrics = "mean_absolute_error", smooth = FALSE)
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(ylim = c(0, 10))
plot(history, metrics = "mean_absolute_error", smooth = FALSE) +
coord_cartesian(ylim = c(0, 5))
elary_stop = callback_early_stopping(monitor = "val_loss", patience = 20)
model = build_model()
elary_stop = callback_early_stopping(monitor = "val_loss", patience = 20)
# TRAIN THE MODEL
# display training progress by printing a single dot for each completed epoch
print_dot_callback = callback_lambda(
on_epoch_end = function(epoch, logs) {
if (epoch %% 80 == 0) cat('\n')
cat('>.<')
}
)
epochs = 500
elary_stop = callback_early_stopping(monitor = "val_loss", patience = 20)
model = build_model()
history = model %>% fit(
train_data, train_labels,
epochs = epochs,
validation_split = 0.2,
verbose = 0,
callbacks = list(early_stop, print_dot_callback)
)
earlyy_stop = callback_early_stopping(monitor = "val_loss", patience = 20)
early_stop = callback_early_stopping(monitor = "val_loss", patience = 20)
model = build_model()
history = model %>% fit(
train_data, train_labels,
epochs = epochs,
validation_split = 0.2,
verbose = 0,
callbacks = list(early_stop, print_dot_callback)
)
plot(history, metrics = 'mean_absolute_error', smooth = TRUE) +
coord_cartesian(xlim = c(0, 150), ylim = c(0, 5))
plot(history, metrics = 'mean_absolute_error', smooth = FALSE) +
coord_cartesian(xlim = c(0, 150), ylim = c(0, 5))
plot(history, metrics = 'mean_absolute_error', smooth = FALSE) +
coord_cartesian(xlim = c(0, 200), ylim = c(0, 5))
# evaluate model's performance with test set
model %>% evaluate(test_data, test_labels)
# predict
model %>% predict(test_data)
# predict
pred = model %>% predict(test_data)
plot(pred, test_labels)
library(keras)
library(dplyr)
library(ggplot2)
library(tidyr)
library(tibble)
# download the IMDB dataset
num_words = 10000
imdb = dataset_imdb(num_words = num_words)
train_data = imdb$train$x
train_labels = imdb$train$y
test_data = imdb$test$x
test_labels = imdb$test$y
# multi-hot-encoding the lists
multi_hot_sequences = function(sequences, dimension) {
multi_hot = matrix(0, nrow = length(sequences), ncol = dimension)
for (i in 1:length(sequences)) {
multi_hot[i, sequences[[i]]] = 1
}
multi_hot
}
train_data = multi_hot_sequences(train_data, num_words)
test_data = multi_hot_sequences(test_data, num_words)
dim(train_data)
dim(test_data)
train_data = imdb$train$x
train_labels = imdb$train$y
test_data = imdb$test$x
test_labels = imdb$test$y
length(test_data)
length(train_data)
# multi-hot-encoding the lists
multi_hot_sequences = function(sequences, dimension) {
multi_hot = matrix(0, nrow = length(sequences), ncol = dimension)
for (i in 1:length(sequences)) {
multi_hot[i, sequences[[i]]] = 1
}
multi_hot
}
train_data = multi_hot_sequences(train_data, num_words)
test_data = multi_hot_sequences(test_data, num_words)
dim(train_data)
dim(test_data)
train_data[1, ]
which(train_data[1, ])
which(train_data[1, ] == 1)
# look at one of the resulting multi-hot vectors
plot(x = 1:10000)
# look at one of the resulting multi-hot vectors
plot(x = 1:10000, yaxt='n')
# look at one of the resulting multi-hot vectors
plot(x = 1:10000, yaxt='n', type='n')
abline(a=which(train_data[1, ] == 1))
abline(a=which(train_data[1, ] == 1), b=0)
abline(b=which(train_data[1, ] == 1), a=0)
b=which(train_data[1, ] == 1)
b
# look at one of the resulting multi-hot vectors
plot(x = 1:10000, yaxt='n', type='n')
abline(h=bwhich(train_data[1, ] == 1))
abline(h=which(train_data[1, ] == 1))
abline(v=which(train_data[1, ] == 1))
# look at one of the resulting multi-hot vectors
plot(x = 1:10000, yaxt='n', type='n')
abline(v=which(train_data[1, ] == 1))
?plot
# look at one of the resulting multi-hot vectors
plot(x = 1:10000, yaxt='n', type='n', frame.plot = FALSE)
abline(v=which(train_data[1, ] == 1))
baseline_model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = list('accuracy')
)
# CREATE A BASELINE MODEL
baseline_model = keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = 10000) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = "sigmoid")
baseline_model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = list('accuracy')
)
baseline_model %>% summary()
baseline_history = baseline_model %>% fit(
train_data, train_labels,
epochs = 20,
batch_size = 512,
validation_data = list(test_data, test_labels),
verbose = 2
)
# CREATE A SMALLER MODEL
smaller_model = keras_model_sequential() %>%
layer_dense(units = 4, activation = 'relu', input_shape = 10000) %>%
layer_dense(units = 4, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
smaller_model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = list('accuracy')
)
smaller_model %>% summary()
smaller_history = smaller_model %>% fit(
train_data, train_labels,
epochs = 20,
batch_size = 512,
validation_data = list(test_data, test_labels),
verbose = 2
)
# CREATE A BIGGER MODEL
bigger_model = keras_model_sequential() %>%
layer_dense(units = 512, activation = 'relu', input_shape = 10000) %>%
layer_dense(units = 512, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
bigger_model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = list('accuracy')
)
bigger_model %>% summary()
bigger_history = bigger_model %>% fit(
train_data, train_labels,
epochs = 20,
batch_size = 512,
validation_data = list(test_data, test_labels),
verbose = 2
)
# PLOT THE TRINING AND VALIDATION LOSS
compare_cx = data.frame(
baseline_train = baseline_history$metrics$loss,
baseline_val = baseline_history$metrics$val_loss,
smaller_train =smaller_history$metrics$loss,
smaller_val = smaller_history$metrics$val_loss,
bigger_train = bigger_history$metrics$loss,
bigger_val = bigger_history$metrics$val_loss
)
compare_cx
plot()
plot(1:nrow(compare_cx), compare_cx[, 1])
plot(1:nrow(compare_cx), compare_cx[, 1], col = 1)
plot(1:nrow(compare_cx), compare_cx[, 1], col = 'red')
plot(1:nrow(compare_cx), compare_cx[, 1], col = 'red', type = 'l')
gather(compare_cx, key='type', value = 'loss')
# PLOT THE TRINING AND VALIDATION LOSS
compare_cx = data.frame(
baseline_train = baseline_history$metrics$loss,
baseline_val = baseline_history$metrics$val_loss,
smaller_train =smaller_history$metrics$loss,
smaller_val = smaller_history$metrics$val_loss,
bigger_train = bigger_history$metrics$loss,
bigger_val = bigger_history$metrics$val_loss,
epochs = 1:20
)
gather(compare_cx, key='type', value = 'loss', -epochs)
# PLOT THE TRINING AND VALIDATION LOSS
compare_cx = data.frame(
baseline_train = baseline_history$metrics$loss,
baseline_val = baseline_history$metrics$val_loss,
smaller_train =smaller_history$metrics$loss,
smaller_val = smaller_history$metrics$val_loss,
bigger_train = bigger_history$metrics$loss,
bigger_val = bigger_history$metrics$val_loss,
epochs = 1:20
) %>%
gather(key='type', value = 'loss', -epochs)
compare_cx
stringr::str_split(compare_cx$type, '_')
stringr::str_split(compare_cx$type, '_', n = 2)
apply(compare_cx, function(x) stringr::str_split(x)[[1]])
tpply(compare_cx, FUN = function(x) stringr::str_split(x)[[1]])
sapply(compare_cx, FUN = function(x) stringr::str_split(x)[[1]])
sapply(compare_cx, FUN = function(x) stringr::str_split(x, '_')[[1]])
sapply(compare_cx$type, FUN = function(x) stringr::str_split(x, '_')[[1]])
tapply(compare_cx$type, FUN = function(x) stringr::str_split(x, '_')[[1]])
sapply(compare_cx$type, FUN = function(x) stringr::str_split(x, '_')[[1]])
stringr::str_split(compare_cx, '_')
stringr::str_split(compare_cx$type, '_')
as.data.frame(stringr::str_split(compare_cx$type, '_'))
t(as.data.frame(stringr::str_split(compare_cx$type, '_')))
stringr::str_split(compare_cx$type, '_'))
stringr::str_split(compare_cx$type, '_')))
stringr::str_split(compare_cx$type, '_')
stringr::str_split(compare_cx$type, '_', 1)
unlist(stringr::str_split(compare_cx$type, '_', 1))
compare_cx$model = unlist(stringr::str_split(compare_cx$type, '_', 1))
unlist(stringr::str_split(compare_cx$type, '_', 1))
unlist(stringr::str_split(compare_cx$type, '*_', 1))
unlist(stringr::str_split(compare_cx$type, '\\*_', 1))
unlist(stringr::str_split(compare_cx$type, '\*_', 1))
unlist(stringr::str_split(compare_cx$type, '\\._', 1))
library(stringr)
str_replace('\\.+_', compare_cx$type)
str_replace(compare_cx$type, '\\.+_', '')
str_replace(compare_cx$type, '\\.\\+_', '')
str_replace(compare_cx$type, '_', '')
str_replace(compare_cx$type, '.+_', '')
# PLOT THE TRINING AND VALIDATION LOSS
compare_cx = data.frame(
baseline_train = baseline_history$metrics$loss,
baseline_val = baseline_history$metrics$val_loss,
smaller_train =smaller_history$metrics$loss,
smaller_val = smaller_history$metrics$val_loss,
bigger_train = bigger_history$metrics$loss,
bigger_val = bigger_history$metrics$val_loss,
epochs = 1:20
) %>%
gather(key='type', value = 'loss', -epochs)
compare_cx$model = str_replace(compare_cx$type, '_.+', '')
compare_cx$type = str_replace(compare_cx$type, '.+_', '')
head(compare_cx)
ggplot(compare_cx, aes(x=epochs, y=loss, color=type, line_type=model)) +
geom_line()
model
compare_cx$model
ggplot(compare_cx, aes(x=epochs, y=loss, color=model, linetype=type)) +
geom_line()
# regularization
l2_model <-
keras_model_sequential() %>%
layer_dense(units = 16, activation = "relu", input_shape = 10000,
kernel_regularizer = regularizer_l2(l = 0.001)) %>%
layer_dense(units = 16, activation = "relu",
kernel_regularizer = regularizer_l2(l = 0.001)) %>%
layer_dense(units = 1, activation = "sigmoid")
l2_model %>% compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = list("accuracy")
)
l2_history <- l2_model %>% fit(
train_data,
train_labels,
epochs = 20,
batch_size = 512,
validation_data = list(test_data, test_labels),
verbose = 2
)
# add dropout
dropout_model <-
keras_model_sequential() %>%
layer_dense(units = 16, activation = "relu", input_shape = 10000) %>%
layer_dropout(0.6) %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.6) %>%
layer_dense(units = 1, activation = "sigmoid")
dropout_model %>% compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = list("accuracy")
)
dropout_history <- dropout_model %>% fit(
train_data,
train_labels,
epochs = 20,
batch_size = 512,
validation_data = list(test_data, test_labels),
verbose = 2
)
# PLOT THE TRINING AND VALIDATION LOSS
compare_cx = data.frame(
baseline_train = baseline_history$metrics$loss,
baseline_val = baseline_history$metrics$val_loss,
l2_train =l2_history$metrics$loss,
l2_val = l2_history$metrics$val_loss,
dropout_train = dropout_history$metrics$loss,
dropout_val = dropout_history$metrics$val_loss,
epochs = 1:20
) %>%
gather(key='type', value = 'loss', -epochs)
compare_cx$model = str_replace(compare_cx$type, '_.+', '')
compare_cx$type = str_replace(compare_cx$type, '.+_', '')
ggplot(compare_cx, aes(x=epochs, y=loss, color=model, linetype=type)) +
geom_line()
# setup
library(keras)
mnist = dataset_mnist()
train_images = mnist$train$x
train_labels = mnist$train$y
test_images = mnist$test$x
test_labels = mnist$test$y
dim(train_images)
train_images = train_images[1:1000,,] %>%
array_reshape(c(1000, 28*28))
train_images = train_images / 255
test_images = test_images[1:1000, , ] %>%
array_reshape(c(1000, 28*28))
test_images = test_images / 255
# BUILD MODEL
create_model = function() {
model = keras_model_sequential() %>%
layer_dense(units = 512, activation = 'relu', input_shape = 28*28) %>%
layer_dropout(0.2) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'sparse_categorical_crossentropy',
metrics = list('accuracy')
)
model
}
model = create_model()
model %>% summary()
# save the entire model
model = create_model()
model %>% fit(
train_images, train_labels,
epochs = 5
)
train_labels <- train_labels[1:1000]
test_labels <- test_labels[1:1000]
model %>% fit(
train_images, train_labels,
epochs = 5
)
model %>% save_model_hdf5('my_model.h5')
# to only save the weights
model %>% save_model_weights_hdf5('my_model_weights.h5')
# now recreate the model from that file
new_model = load_model_hdf5('my_model.h5')
new_model %>% summary()
# save checkpoints during training
checkpoint_dir = "checkpoints"
dir.create(checkpoint_dir, showWarnings = FALSE)
filepath = file.path(checkpoint_dir, "weights.{epoch:02d}-{val_loss:.2f}.hd5")
cp_callback = callback_model_checkpoint(
filepath = filepath,
save_best_only = TRUE,
verbose = 1
)
model = create_model()
model %>% fit(
train_images, train_labels,
epochs = 10,
validation_data = list(test_images, test_labels),
callbacks = list(cp_callback) # pass callback to training
)
# inspect the files that were created
list.files(checkpoint_dir)
# now rebuild afresh, untrained model and evaluate it on the test set
fresh_model = create_model()
score = fresh_model %>% evaluate(test_images, test_labels)
cat('Test lost:', score$loss, '\n')
cat('Test accuracy:', score$acc, '\n')
list.files(checkpoint_dir)
# then load the weights from the latest checkpoint (epoch 10), and re-evaluate:
fresh_model %>% load_model_weights_hdf5(
file.path(checkpoint_dir, "weights.08-0.39.hd5")
)
score = fresh_model %>% evaluate(test_images, test_labels)
cat('Test lost:', score$loss, '\n')
cat('Test accuracy:', score$acc, '\n')
unlink(checkpoint_dir, recursive = TRUE)
# to reduce the number of files, you can also save model weights only once every nth epoch.
checkpoint_dir = "checkpoints"
unlink(checkpoint_dir, recursive = TRUE)
dir.create(checkpoint_dir)
filepath <- file.path(checkpoint_dir, "weights.{epoch:02d}-{val_loss:.2f}.hdf5")
# Create checkpoint callback
cp_callback <- callback_model_checkpoint(
filepath = filepath,
save_weights_only = TRUE,
period = 5,
verbose = 1
)
model <- create_model()
model %>% fit(
train_images,
train_labels,
epochs = 10,
validation_data = list(test_images, test_labels),
callbacks = list(cp_callback)  # pass callback to training
)
list.files(checkpoint_dir)
# alternatively, you can also decide to save only the best model
checkpoint_dir <- "checkpoints"
unlink(checkpoint_dir, recursive = TRUE)
dir.create(checkpoint_dir)
filepath <- file.path(checkpoint_dir, "weights.{epoch:02d}-{val_loss:.2f}.hdf5")
# Create checkpoint callback
cp_callback <- callback_model_checkpoint(
filepath = filepath,
save_weights_only = TRUE,
save_best_only = TRUE,
verbose = 1
)
model <- create_model()
model %>% fit(
train_images,
train_labels,
epochs = 10,
validation_data = list(test_images, test_labels),
callbacks = list(cp_callback)  # pass callback to training
)
list.files(checkpoint_dir)
