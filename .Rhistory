epochs = 1:20
)
gather(compare_cx, key='type', value = 'loss', -epochs)
# PLOT THE TRINING AND VALIDATION LOSS
compare_cx = data.frame(
baseline_train = baseline_history$metrics$loss,
baseline_val = baseline_history$metrics$val_loss,
smaller_train =smaller_history$metrics$loss,
smaller_val = smaller_history$metrics$val_loss,
bigger_train = bigger_history$metrics$loss,
bigger_val = bigger_history$metrics$val_loss,
epochs = 1:20
) %>%
gather(key='type', value = 'loss', -epochs)
compare_cx
stringr::str_split(compare_cx$type, '_')
stringr::str_split(compare_cx$type, '_', n = 2)
apply(compare_cx, function(x) stringr::str_split(x)[[1]])
tpply(compare_cx, FUN = function(x) stringr::str_split(x)[[1]])
sapply(compare_cx, FUN = function(x) stringr::str_split(x)[[1]])
sapply(compare_cx, FUN = function(x) stringr::str_split(x, '_')[[1]])
sapply(compare_cx$type, FUN = function(x) stringr::str_split(x, '_')[[1]])
tapply(compare_cx$type, FUN = function(x) stringr::str_split(x, '_')[[1]])
sapply(compare_cx$type, FUN = function(x) stringr::str_split(x, '_')[[1]])
stringr::str_split(compare_cx, '_')
stringr::str_split(compare_cx$type, '_')
as.data.frame(stringr::str_split(compare_cx$type, '_'))
t(as.data.frame(stringr::str_split(compare_cx$type, '_')))
stringr::str_split(compare_cx$type, '_'))
stringr::str_split(compare_cx$type, '_')))
stringr::str_split(compare_cx$type, '_')
stringr::str_split(compare_cx$type, '_', 1)
unlist(stringr::str_split(compare_cx$type, '_', 1))
compare_cx$model = unlist(stringr::str_split(compare_cx$type, '_', 1))
unlist(stringr::str_split(compare_cx$type, '_', 1))
unlist(stringr::str_split(compare_cx$type, '*_', 1))
unlist(stringr::str_split(compare_cx$type, '\\*_', 1))
unlist(stringr::str_split(compare_cx$type, '\*_', 1))
unlist(stringr::str_split(compare_cx$type, '\\._', 1))
library(stringr)
str_replace('\\.+_', compare_cx$type)
str_replace(compare_cx$type, '\\.+_', '')
str_replace(compare_cx$type, '\\.\\+_', '')
str_replace(compare_cx$type, '_', '')
str_replace(compare_cx$type, '.+_', '')
# PLOT THE TRINING AND VALIDATION LOSS
compare_cx = data.frame(
baseline_train = baseline_history$metrics$loss,
baseline_val = baseline_history$metrics$val_loss,
smaller_train =smaller_history$metrics$loss,
smaller_val = smaller_history$metrics$val_loss,
bigger_train = bigger_history$metrics$loss,
bigger_val = bigger_history$metrics$val_loss,
epochs = 1:20
) %>%
gather(key='type', value = 'loss', -epochs)
compare_cx$model = str_replace(compare_cx$type, '_.+', '')
compare_cx$type = str_replace(compare_cx$type, '.+_', '')
head(compare_cx)
ggplot(compare_cx, aes(x=epochs, y=loss, color=type, line_type=model)) +
geom_line()
model
compare_cx$model
ggplot(compare_cx, aes(x=epochs, y=loss, color=model, linetype=type)) +
geom_line()
# regularization
l2_model <-
keras_model_sequential() %>%
layer_dense(units = 16, activation = "relu", input_shape = 10000,
kernel_regularizer = regularizer_l2(l = 0.001)) %>%
layer_dense(units = 16, activation = "relu",
kernel_regularizer = regularizer_l2(l = 0.001)) %>%
layer_dense(units = 1, activation = "sigmoid")
l2_model %>% compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = list("accuracy")
)
l2_history <- l2_model %>% fit(
train_data,
train_labels,
epochs = 20,
batch_size = 512,
validation_data = list(test_data, test_labels),
verbose = 2
)
# add dropout
dropout_model <-
keras_model_sequential() %>%
layer_dense(units = 16, activation = "relu", input_shape = 10000) %>%
layer_dropout(0.6) %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.6) %>%
layer_dense(units = 1, activation = "sigmoid")
dropout_model %>% compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = list("accuracy")
)
dropout_history <- dropout_model %>% fit(
train_data,
train_labels,
epochs = 20,
batch_size = 512,
validation_data = list(test_data, test_labels),
verbose = 2
)
# PLOT THE TRINING AND VALIDATION LOSS
compare_cx = data.frame(
baseline_train = baseline_history$metrics$loss,
baseline_val = baseline_history$metrics$val_loss,
l2_train =l2_history$metrics$loss,
l2_val = l2_history$metrics$val_loss,
dropout_train = dropout_history$metrics$loss,
dropout_val = dropout_history$metrics$val_loss,
epochs = 1:20
) %>%
gather(key='type', value = 'loss', -epochs)
compare_cx$model = str_replace(compare_cx$type, '_.+', '')
compare_cx$type = str_replace(compare_cx$type, '.+_', '')
ggplot(compare_cx, aes(x=epochs, y=loss, color=model, linetype=type)) +
geom_line()
# setup
library(keras)
mnist = dataset_mnist()
train_images = mnist$train$x
train_labels = mnist$train$y
test_images = mnist$test$x
test_labels = mnist$test$y
dim(train_images)
train_images = train_images[1:1000,,] %>%
array_reshape(c(1000, 28*28))
train_images = train_images / 255
test_images = test_images[1:1000, , ] %>%
array_reshape(c(1000, 28*28))
test_images = test_images / 255
# BUILD MODEL
create_model = function() {
model = keras_model_sequential() %>%
layer_dense(units = 512, activation = 'relu', input_shape = 28*28) %>%
layer_dropout(0.2) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
optimizer = 'adam',
loss = 'sparse_categorical_crossentropy',
metrics = list('accuracy')
)
model
}
model = create_model()
model %>% summary()
# save the entire model
model = create_model()
model %>% fit(
train_images, train_labels,
epochs = 5
)
train_labels <- train_labels[1:1000]
test_labels <- test_labels[1:1000]
model %>% fit(
train_images, train_labels,
epochs = 5
)
model %>% save_model_hdf5('my_model.h5')
# to only save the weights
model %>% save_model_weights_hdf5('my_model_weights.h5')
# now recreate the model from that file
new_model = load_model_hdf5('my_model.h5')
new_model %>% summary()
# save checkpoints during training
checkpoint_dir = "checkpoints"
dir.create(checkpoint_dir, showWarnings = FALSE)
filepath = file.path(checkpoint_dir, "weights.{epoch:02d}-{val_loss:.2f}.hd5")
cp_callback = callback_model_checkpoint(
filepath = filepath,
save_best_only = TRUE,
verbose = 1
)
model = create_model()
model %>% fit(
train_images, train_labels,
epochs = 10,
validation_data = list(test_images, test_labels),
callbacks = list(cp_callback) # pass callback to training
)
# inspect the files that were created
list.files(checkpoint_dir)
# now rebuild afresh, untrained model and evaluate it on the test set
fresh_model = create_model()
score = fresh_model %>% evaluate(test_images, test_labels)
cat('Test lost:', score$loss, '\n')
cat('Test accuracy:', score$acc, '\n')
list.files(checkpoint_dir)
# then load the weights from the latest checkpoint (epoch 10), and re-evaluate:
fresh_model %>% load_model_weights_hdf5(
file.path(checkpoint_dir, "weights.08-0.39.hd5")
)
score = fresh_model %>% evaluate(test_images, test_labels)
cat('Test lost:', score$loss, '\n')
cat('Test accuracy:', score$acc, '\n')
unlink(checkpoint_dir, recursive = TRUE)
# to reduce the number of files, you can also save model weights only once every nth epoch.
checkpoint_dir = "checkpoints"
unlink(checkpoint_dir, recursive = TRUE)
dir.create(checkpoint_dir)
filepath <- file.path(checkpoint_dir, "weights.{epoch:02d}-{val_loss:.2f}.hdf5")
# Create checkpoint callback
cp_callback <- callback_model_checkpoint(
filepath = filepath,
save_weights_only = TRUE,
period = 5,
verbose = 1
)
model <- create_model()
model %>% fit(
train_images,
train_labels,
epochs = 10,
validation_data = list(test_images, test_labels),
callbacks = list(cp_callback)  # pass callback to training
)
list.files(checkpoint_dir)
# alternatively, you can also decide to save only the best model
checkpoint_dir <- "checkpoints"
unlink(checkpoint_dir, recursive = TRUE)
dir.create(checkpoint_dir)
filepath <- file.path(checkpoint_dir, "weights.{epoch:02d}-{val_loss:.2f}.hdf5")
# Create checkpoint callback
cp_callback <- callback_model_checkpoint(
filepath = filepath,
save_weights_only = TRUE,
save_best_only = TRUE,
verbose = 1
)
model <- create_model()
model %>% fit(
train_images,
train_labels,
epochs = 10,
validation_data = list(test_images, test_labels),
callbacks = list(cp_callback)  # pass callback to training
)
list.files(checkpoint_dir)
library(keras)
# use the keras function api to build complex model topologies such as
# multi-input models,
# multi-output models,
# models with shared layers (the same layer called several times)
# models with non-sequential data flows (e.g., residual connections)
inputs = layer_input(shape = (32))
predictions = inputs %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 10, activation = 'softmax')
# instantiate the model diven inputs and outputs
model = keras_model(inputs = inputs, outputs = predictions)
# the compile step specifies the training configuration
model %>% compile(
optimizer = optimizer_rmsprop(lr = 0.001),
loss = 'categorical_crossentropy',
metrics = list('accuracy')
)
# trains for 5 epochs
model %>% fit(
data,
labels,
batch_size = 32,
epochs = 5
)
data <- matrix(rnorm(1000 * 32), nrow = 1000, ncol = 32)
labels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)
# trains for 5 epochs
model %>% fit(
data,
labels,
batch_size = 32,
epochs = 5
)
library(httr)
# to make a request
GET("http://httpbin.org/get")
# to make a request
r = GET("http://httpbin.org/get")
r
# you can pull out important parts of the response with various helper
# methods, or dig directly into the object
status_code(r)
headers(r)
# THE RESPONSE
# the data sent back from the server consists of three parts:
# 1) the status line
# 2) the headers
# 3) the body
# the most important part of the status line is the http status code
# it tells you whether or not the request was successful
# you can access the status code along with a descriptive message using
# http_status
http_status(r)
r$status_code
# THE BODY
# three ways to access the body of the request, all using content
# 1) access the body as a character vector
# 2) access the body as a raw vector
# 3) httr provides a number of default parsers for common file types
content(r, "text")
# THE BODY
# three ways to access the body of the request, all using content
# 1) access the body as a character vector
# 2) access the body as a raw vector
# 3) httr provides a number of default parsers for common file types
content(r, "text", encoding = 'ISO-8859-1')
content(r, 'raw')
str(content(r, 'parsed'))
# THE HEADERS
headers(r)
# Cookies
cookies(r)
# Cookies
r = GET("http://httpbin.org/cookies/set", query = list(a = 1))
cookies(r)
# cookies are automatically persisted between requests to the same domain
r = GET("http://httpbin.org/cookies/set", query = list(b = 1))
cookies(r)
r = GET("http://httpbin.org/get",
query = list(key1 = "value1", key2 = "value2")
)
content(r)$args
content(r)$args
# any NULL elements are automatically dropped from the list
r <- GET("http://httpbin.org/get",
query = list(key1 = "value 1", "key 2" = "value2", key2 = NULL))
content(r)$args
# you can add custom headers to a request with add_headers(
# custom headers
# you can add custom headers to a request with add_headers()
r <- GET("http://httpbin.org/get", add_headers(Name = "Hadley"))
str(content(r)$headers)
# cookies are simple key-value pairs like the query string, but they persist across
# multiple requests in a session
r <- GET("http://httpbin.org/cookies", set_cookies("MeWant" = "cookies"))
content(r)$cookies
# request body
# when POST()  ing, you can include data in the body of the request
# httr allows you to supply this in a number of different ways, the most
# common way is a named list
r = POST('http://httpbin.org/post', body = list(a=1, b=2, c=3))
# you can use the encode argument to determine how this data is sent to the server
url <- "http://httpbin.org/post"
body <- list(a = 1, b = 2, c = 3)
# Form encoded
r <- POST(url, body = body, encode = "form")
# Multipart encoded
r <- POST(url, body = body, encode = "multipart")
# JSON encoded
r <- POST(url, body = body, encode = "json")
POST(url, body = body, encode = "multipart", verbose()) # the default
POST(url, body = body, encode = "form", verbose())
POST(url, body = body, encode = "json", verbose())
POST(url, body = body, encode = "multipart", verbose()) # the default
library(httr)
# FIRST STEPS
# Send a simple request
# first, find a simple API endpoint that doesn't require authentication
# for this example, we'll use the list of httr issues which requires sending
# a GET request to repos/hadley/httr
github_api = function(path) {
url = modify_url("https://api.github.com", path = path)
GET(url)
}
resp = github_api("/repos/hadley/httr")
resp
# PARSE THE RESPONSE
# next, you need to take the response returned by the API and turn it into a useful
# object. Any API with return an HTTP response that consists of headers and a body.
# while the response can come in multiple forms (see above), two of the most common
# structured formats are XML and JSON
GET("http://www.colourlovers.com/api/color/6B4106?format=xml")
GET("http://www.colourlovers.com/api/color/6B4106?format=json")
# if you have a choice, choose json: it's usually much easier to work with than xml
# to determine what type of content is returned
http_type(resp)
# I recommend checking that the type is as you expect in your helper function.
# this will ensure that you get a clear error message if the API changes:
github_api = function(path) {
url <- modify_url("https://api.github.com", path = path)
resp <- GET(url)
if (http_type(resp) != "application/json") {
stop("API did not return json", call. = FALSE)
}
resp
}
# next we need to parse the output into an R object. httr provides some default parsers with
# content(..., as = "auto") but i don't recommend using them inside a package. Instead it's
# better to explicityly parse it yourself.
github_api = function(path) {
url <- modify_url("https://api.github.com", path = path)
resp <- GET(url)
if (http_type(resp) != "application/json") {
stop("API did not return json", call. = FALSE)
}
jsonlite::fromJSON(content(resp, "text"), simplifyVector = FALSE)
}
print.github_api = function(x, ...) {
cat("<GitHub ", x$path, ">\n", sep = "")
str(x$content)
invisible(x)
}
github_api("/users/hadley")
print(github_api("/users/hadley"))
# Turn API errors into R errors
# next, you need to make sure that your API wrapper throws an error if the
# request failed. Using a web API introduces additional possible points of
# failure into R code aside from those occuring in R itself. These include:
# 1. client-side exceptions
# 2. network / communication exceptions
# 3. server-side exceptions
github_api <- function(path) {
url <- modify_url("https://api.github.com", path = path)
resp <- GET(url)
if (http_type(resp) != "application/json") {
stop("API did not return json", call. = FALSE)
}
parsed <- jsonlite::fromJSON(content(resp, "text"), simplifyVector = FALSE)
if (http_error(resp)) {
stop(
sprintf(
"GitHub API request failed [%s]\n%s\n<%s>",
status_code(resp),
parsed$message,
parsed$documentation_url
),
call. = FALSE
)
}
structure(
list(
content = parsed,
path = path,
response = resp
),
class = "github_api"
)
}
github_api("/user/hadley")
# SET A USER AGENT
# a good default for an R API package wrapper is to make it the URL to your GitHub repo:
ua = user_agent("http://github.com/hadley/httr")
ua
github_api <- function(path) {
url <- modify_url("https://api.github.com", path = path)
resp <- GET(url, ua)
if (http_type(resp) != "application/json") {
stop("API did not return json", call. = FALSE)
}
parsed <- jsonlite::fromJSON(content(resp, "text"), simplifyVector = FALSE)
if (status_code(resp) != 200) {
stop(
sprintf(
"GitHub API request failed [%s]\n%s\n<%s>",
status_code(resp),
parsed$message,
parsed$documentation_url
),
call. = FALSE
)
}
structure(
list(
content = parsed,
path = path,
response = resp
),
class = "github_api"
)
}
# PASSING PARAMETERS
# 1. URL path: modify_url()
# 2. Query arguments: The query argument to GET(), POST(), etc.
# 3. HTTP headers: add_headers()
# 4. Request body: The body argument to GET(), POST(), etc.
# modify_url
POST(modify_url("https://httpbin.org", path = "/post"))
# query arguments
POST("http://httpbin.org/post", query = list(foo = "bar"))
# headers
POST("http://httpbin.org/post", add_headers(foo = "bar"))
# body
## as form
POST("http://httpbin.org/post", body = list(foo = "bar"), encode = "form")
## as json
POST("http://httpbin.org/post", body = list(foo = "bar"), encode = "json")
# Rate limiting
rate_limit <- function() {
github_api("/rate_limit")
}
rate_limit()
rate_limit <- function() {
req <- github_api("/rate_limit")
core <- req$content$resources$core
reset <- as.POSIXct(core$reset, origin = "1970-01-01")
cat(core$remaining, " / ", core$limit,
" (Resets at ", strftime(reset, "%H:%M:%S"), ")\n", sep = "")
}
rate_limit()
rate_limit()
rate_limit()
# Rate limiting
rate_limit <- function() {
github_api("/rate_limit")
}
rate_limit()
library(keras)
