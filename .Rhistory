# cookies are automatically persisted between requests to the same domain
r = GET("http://httpbin.org/cookies/set", query = list(b = 1))
cookies(r)
r = GET("http://httpbin.org/get",
query = list(key1 = "value1", key2 = "value2")
)
content(r)$args
content(r)$args
# any NULL elements are automatically dropped from the list
r <- GET("http://httpbin.org/get",
query = list(key1 = "value 1", "key 2" = "value2", key2 = NULL))
content(r)$args
# you can add custom headers to a request with add_headers(
# custom headers
# you can add custom headers to a request with add_headers()
r <- GET("http://httpbin.org/get", add_headers(Name = "Hadley"))
str(content(r)$headers)
# cookies are simple key-value pairs like the query string, but they persist across
# multiple requests in a session
r <- GET("http://httpbin.org/cookies", set_cookies("MeWant" = "cookies"))
content(r)$cookies
# request body
# when POST()  ing, you can include data in the body of the request
# httr allows you to supply this in a number of different ways, the most
# common way is a named list
r = POST('http://httpbin.org/post', body = list(a=1, b=2, c=3))
# you can use the encode argument to determine how this data is sent to the server
url <- "http://httpbin.org/post"
body <- list(a = 1, b = 2, c = 3)
# Form encoded
r <- POST(url, body = body, encode = "form")
# Multipart encoded
r <- POST(url, body = body, encode = "multipart")
# JSON encoded
r <- POST(url, body = body, encode = "json")
POST(url, body = body, encode = "multipart", verbose()) # the default
POST(url, body = body, encode = "form", verbose())
POST(url, body = body, encode = "json", verbose())
POST(url, body = body, encode = "multipart", verbose()) # the default
library(httr)
# FIRST STEPS
# Send a simple request
# first, find a simple API endpoint that doesn't require authentication
# for this example, we'll use the list of httr issues which requires sending
# a GET request to repos/hadley/httr
github_api = function(path) {
url = modify_url("https://api.github.com", path = path)
GET(url)
}
resp = github_api("/repos/hadley/httr")
resp
# PARSE THE RESPONSE
# next, you need to take the response returned by the API and turn it into a useful
# object. Any API with return an HTTP response that consists of headers and a body.
# while the response can come in multiple forms (see above), two of the most common
# structured formats are XML and JSON
GET("http://www.colourlovers.com/api/color/6B4106?format=xml")
GET("http://www.colourlovers.com/api/color/6B4106?format=json")
# if you have a choice, choose json: it's usually much easier to work with than xml
# to determine what type of content is returned
http_type(resp)
# I recommend checking that the type is as you expect in your helper function.
# this will ensure that you get a clear error message if the API changes:
github_api = function(path) {
url <- modify_url("https://api.github.com", path = path)
resp <- GET(url)
if (http_type(resp) != "application/json") {
stop("API did not return json", call. = FALSE)
}
resp
}
# next we need to parse the output into an R object. httr provides some default parsers with
# content(..., as = "auto") but i don't recommend using them inside a package. Instead it's
# better to explicityly parse it yourself.
github_api = function(path) {
url <- modify_url("https://api.github.com", path = path)
resp <- GET(url)
if (http_type(resp) != "application/json") {
stop("API did not return json", call. = FALSE)
}
jsonlite::fromJSON(content(resp, "text"), simplifyVector = FALSE)
}
print.github_api = function(x, ...) {
cat("<GitHub ", x$path, ">\n", sep = "")
str(x$content)
invisible(x)
}
github_api("/users/hadley")
print(github_api("/users/hadley"))
# Turn API errors into R errors
# next, you need to make sure that your API wrapper throws an error if the
# request failed. Using a web API introduces additional possible points of
# failure into R code aside from those occuring in R itself. These include:
# 1. client-side exceptions
# 2. network / communication exceptions
# 3. server-side exceptions
github_api <- function(path) {
url <- modify_url("https://api.github.com", path = path)
resp <- GET(url)
if (http_type(resp) != "application/json") {
stop("API did not return json", call. = FALSE)
}
parsed <- jsonlite::fromJSON(content(resp, "text"), simplifyVector = FALSE)
if (http_error(resp)) {
stop(
sprintf(
"GitHub API request failed [%s]\n%s\n<%s>",
status_code(resp),
parsed$message,
parsed$documentation_url
),
call. = FALSE
)
}
structure(
list(
content = parsed,
path = path,
response = resp
),
class = "github_api"
)
}
github_api("/user/hadley")
# SET A USER AGENT
# a good default for an R API package wrapper is to make it the URL to your GitHub repo:
ua = user_agent("http://github.com/hadley/httr")
ua
github_api <- function(path) {
url <- modify_url("https://api.github.com", path = path)
resp <- GET(url, ua)
if (http_type(resp) != "application/json") {
stop("API did not return json", call. = FALSE)
}
parsed <- jsonlite::fromJSON(content(resp, "text"), simplifyVector = FALSE)
if (status_code(resp) != 200) {
stop(
sprintf(
"GitHub API request failed [%s]\n%s\n<%s>",
status_code(resp),
parsed$message,
parsed$documentation_url
),
call. = FALSE
)
}
structure(
list(
content = parsed,
path = path,
response = resp
),
class = "github_api"
)
}
# PASSING PARAMETERS
# 1. URL path: modify_url()
# 2. Query arguments: The query argument to GET(), POST(), etc.
# 3. HTTP headers: add_headers()
# 4. Request body: The body argument to GET(), POST(), etc.
# modify_url
POST(modify_url("https://httpbin.org", path = "/post"))
# query arguments
POST("http://httpbin.org/post", query = list(foo = "bar"))
# headers
POST("http://httpbin.org/post", add_headers(foo = "bar"))
# body
## as form
POST("http://httpbin.org/post", body = list(foo = "bar"), encode = "form")
## as json
POST("http://httpbin.org/post", body = list(foo = "bar"), encode = "json")
# Rate limiting
rate_limit <- function() {
github_api("/rate_limit")
}
rate_limit()
rate_limit <- function() {
req <- github_api("/rate_limit")
core <- req$content$resources$core
reset <- as.POSIXct(core$reset, origin = "1970-01-01")
cat(core$remaining, " / ", core$limit,
" (Resets at ", strftime(reset, "%H:%M:%S"), ")\n", sep = "")
}
rate_limit()
rate_limit()
rate_limit()
# Rate limiting
rate_limit <- function() {
github_api("/rate_limit")
}
rate_limit()
library(keras)
set.seed(123)
# get ames housing data
library(rsample)
# get ames housing data
ames_housing = AmesHousing::make_ames()
ames_housing
dim(ames_housing)
str(ames_housing)
# split data
library(rsample)
ames_split = initial_split(ames_housing, prop = 0.7)
str(ames_split)
dim(ames_housing)
dim(ames_split)
ames_train = training(ames_split)
ames_test = testing(ames_split)
# RandomForest package
# for reproducibility
set.seed(123)
library(randomForest)
m1 = randomForest(formula = Sale_Price ~ ., data = ames_train)
m1
plot(m1)
summary(m1)
m1$importance
plot(m1$importance)
barplot(m1$importance)
summary(m1)
which.min(m1$mse)
# RMSE of the optimal random forest
sqrt(m1$mse[which.min(m1$mse)])
library(tidyverse)
library(cluster)
install.packages('factoextra')
library(factoextra)
df = USArrests
dim(df)
# remove any missing value
df = na.omit(df)
# scale data
df = scale(df)
head(df)
distance = get_dist(df)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
k2 = kmeans(df, centers = 2, nstart = 25)
str(k2)
k2
fviz_cluster(k2, data = df)
df
library(rsample)
library(randomForest)
library(ranger)
install.packages('ranger')
library(ranger)
library(caret)
library(h2o)
set.seed(123)
library(AmesHousing)
library(AmesHousing)
ames_split = initial_split(make_ames(), prop = .7)
ames_train = train(ames_split)
ames_test = test(ames_split)
ames_train = training(ames_split)
ames_test = testing(ames_split)
dim(ames_train)
dim(ames_test)
# for reproducibility
set.seed(123)
m1 <- randomForest(
formula = Sale_Price ~ .,
data = ames_train
)
m1
plot(m1)
which.min(m1$mse)
m1$mse[which.min(m1$mse)]
sqrt(m1$mse[which.min(m1$mse)])
# randomeForest also allows us to use a validation set to measure prediction
set.seed(123)
valid_split = initial_split(ames_train, .8)
# training_data
ames_train_v2 = analysis(valid_split)
# validation data
ames_valid = assessment(valid_split)
dim(ames_train_v2)
dim(ames_valid)
x_test = ames_valid[, -"Sale_Price"]
x_test = ames_valid[, -c("Sale_Price")]
ames_valid
x_test = ames_valid[, -c("Sale_Price")]
x_test = ames_valid[, -"Sale_Price"]
setdiff(names(ames_valid), "Sale_Price")
x_test = ames_valid[, "Sale_Price" %in% names(ames_valid)]
dim(x_test)
"Sale_Price" %in% names(ames_valid)
x_test = ames_valid[, names(ames_valid) %in% "Sale_Price"]
dim(x_test)
x_test = ames_valid[, names(ames_valid) !%in% "Sale_Price"]
x_test = ames_valid[, !(names(ames_valid) %in% "Sale_Price")]
dim(x_test)
y_test = ames_valid$Sale_Price
library(rsample)
library(caret)
library(h2o)
library(dplyr)
h2o.no_progress() # turn off progress bars
# get some data
library(AmesHousing)
# data()
ames = make_ames()
# splitting data
# typical training-testing splits include 60:40 and 70:30
train_index = createDataPartition(ames$Sale_Price, p = 0.7, list = FALSE)
train_index
# different ways to split the data into training and testing sets
# base R
index = sample(1:nrow(ames), round(nrow(ames) * 0.7))
train_1 = ames[index, ]
test_1 = ames[-index, ]
# caret package
set.seed(123)
index2 = createDataPartition(y = ames$Sale_Price, p = 0.7, list = FALSE)
train_2 = ames[index2, ]
test_2 = ames[-index2, ]
# rsample package
split_1 = initial_split(ames, prop = 0.7)
train_3 = training(split_1)
test_3 = testing(split_1)
# h2o package
h2o.init()
ames_h2o = as.h2o(ames)
split_2 = h2o.splitFrame(ames_h2o, ratios = 0.7, seed = 123)
train_4 = split_2[[1]]
test_4 = split_2[[2]]
str(split_2)
length(split2)
length(split_2)
library(ggplot2)
layout(matrix(1:4, nrow=1)
layout(matrix(1:4, nrow=1)
)
layout(matrix(1:4, nrow=1))
density(train_1$Sale_Price)
plot(train_1$Sale_Price)
plot(train_1$Sale_Price, type = 'density')
?density
plot(density(train_1$Sale_Price))
plot(density(train_1$Sale_Price), col='red')
plot(density(train_1$Sale_Price), col='red')
plot(density(train_2$Sale_Price), col='red', add = TRUE)
plot(density(train_1$Sale_Price), col='red', add = TRUE)
# stratified sampling
data(churn)
warnings()
# stratified sampling
attrition
# stratified sampling
dim(attrition)
table(attrition$Attrition)
table(attrition$Attrition) %>% prop.table()
# stratified sampling with rsample package
set.seed(123)
split_strat = initial_split(attrition, prop = .7, strata = "Attrition")
train_strat = training(split_strat)
test_strat = testing(split_strat)
table(train_strat$Attrition) %>% prop.table()
table(test_strat$Attrition) %>% prop.table()
rep(letters[1:3], 8)
rep(letters[1:3], each=8)
rep(letters[1:3], length=8)
# transform categorical variables into numeric representations
# many ways:
# one-hot encoding, ordinal, binary, sum, Helmert
df = data.frame(id=1:8, x = rep(letters[1:3], length=8))
df
full_rank = dummyVars(~ . data = df, fullRank = TRUE)
dummyVars(~ . data = df, fullRank = TRUE)
full_rank = dummyVars(~ .,  data = df, fullRank = TRUE)
dummyVars(~ . data = df, fullRank = TRUE)
full_rank = dummyVars(~ .,  data = df, fullRank = TRUE)
full_rank
predict(full_rank, df)
?dummyVars
ggplot(data = ames, mapping = aes(x = Sale_Price)) +
geom_density()
ggplot(data = ames, mapping = aes(x = Sale_Price)) +
geom_density(trim = TRUE)
ggplot(data = ames, mapping = aes(x = Sale_Price)) +
geom_density() +
geom_density(data = test_1, col = 'red')
# response transformation
# opt 1: log transformation
# opt 2: Box Cox transformation
log(train_1$Sale_Price)
lambda = forecast::BoxCox(train_1$Sale_Price)
lambda = forecast::BoxCox.lambda(train_1$Sale_Price)
lambda
library(forecast)
?BoxCox.lambda
# Predictor transformation
features = setdiff(names(train_1), "Sale_Price")
pre_process = preProcess(
x = train_1[, features],
method = c('center', 'scale')
)
# apply to both training and test
train_x = predict(pre_process, train_1[, features])
test_x = predict(pre_process, test_1[, features])
library(tidyverse)
library(modelr)
library(broom)
advertising = read_csv("http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv")
advertising
advertising = advertising %>%
select(-X1)
# preparing out data
set.seed(123)
library(rsample)
advertising
my_split = initial_split(data = advertising, prop = 0.7)
train = training(my_split)
test = testing(my_split)
# model building
model1 = lm(Sales ~ TV, data = train)
train
# model building
model1 = lm(sales ~ TV, data = train)
plot(model1)
plot(model1)
summary(model1)
tidy(model1)
confint(model1)
6.7413514 + 2*0.535638206
6.74135142 + 2*0.535638206
summary(model1)
predict(model1)
model1$residuals
sum(model1$residuals)
sum(model1$residuals^2)
mean(model1$residuals^2)
predict(model1, newdata = test)
(test$sales - predict(model1, newdata = test))
mean((test$sales - predict(model1, newdata = test))^2)
# model building
model1 = lm(sales ~ TV, data = train)
summary(model1)
tidy(model1)
model_summary = tidy(model1)
model_summary$estimate - 1.96 * model_summary$std.error
confint(model1)
model_summary$estimate - 2 * model_summary$std.error
model_summary$estimate - 1.96 * model_summary$std.error
model_summary$estimate + 1.96 * model_summary$std.error
tidy(model1)
summary(model1)
sqrt((model1$residuals^2)/(n-2))
(model1$residuals^2)
sqrt(sum(model1$residuals^2)/(n-2))
sqrt(sum(model1$residuals^2))
sqrt(sum(model1$residuals^2)/(length(model1$residuals - 2)))
summary(model1)
TSS = var(train$sales)
TSS
RSS = (model1$residuals)^2
p = 1
n = length(train$sales)
F = ( (TSS - RSS)/1 )/(RS/(n - p - 1))
F = ( (TSS - RSS)/1 )/(RSS/(n - p - 1))
F
TSS = var(train$sales)
RSS = sum((model1$residuals)^2)
p = 1
n = length(train$sales)
F = ( (TSS - RSS)/1 )/(RSS/(n - p - 1))
F
RSS
model1$residuals^2
sum(model1$residuals^2)
TSS = sum((train$sales - mean(train$sales))^2)
RSS = sum(model1$residuals^2)
TSS
p = 1
n = length(train$sales)
F = ( (TSS - RSS)/1 )/(RSS/(n - p - 1))
F
# assessing our model visually
ggplot(train, aes(TV, sales)) +
geom_point() +
geom_smooth(method = "lm")
# assessing our model visually
ggplot(train, aes(TV, sales)) +
geom_point() +
geom_smooth(method = "lm") +
geom_smooth(se = FALSE, color = "red")
augment(model1, train)
# first plot is fitted value vs residuals
model1_results = augment(model1, train)
ggplot(model1_results, aes(.fitted, .resid)) +
geom_ref_line(h = 0)
ggplot(model1_results, aes(.fitted, .resid)) +
geom_ref_line(h = 0) +
geom_point() +
geom_smooth(se = FALSE) +
ggtitle("Residuals vs Fitted")
plot(model1, which = 3)
plot(model1, which = 1)
plot(model1, which = 2)
plot(model1, which = 4)
plot(model1, which = 5)
# multiple linear regression
model2 = lm(sales ~ TV + radio + newspaper, data = train)
model2
summary(model2)
tidy(model2)
confint(model2)
list(model1 = broom::glance(model1), model2 = broom::glance(model2))
list(model1 = broom::glance(model1), model2 = broom::glance(model2))
list(model1 = broom::glance(model1), model2 = broom::glance(model2))
plot(model2)
plot(model2, which = 4)
